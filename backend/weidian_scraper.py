import os
import requests
import re
import json
import time
import logging
import concurrent.futures
from urllib.parse import urlparse, parse_qs, quote
from typing import Dict, List, Optional

from config import config

logger = logging.getLogger(__name__)

COOKIE_FILE = os.path.join(config.DATA_DIR, 'weidian_cookies.txt')


def load_cookie_string() -> Optional[str]:
    try:
        with open(COOKIE_FILE, 'r', encoding='utf-8') as f:
            value = f.read().strip()
            return value if value else None
    except FileNotFoundError:
        return None
    except Exception as e:
        logger.warning(f"åŠ è½½Cookieså¤±è´¥: {e}")
        return None


def save_cookie_string(cookie_string: str) -> bool:
    try:
        os.makedirs(os.path.dirname(COOKIE_FILE), exist_ok=True)
        with open(COOKIE_FILE, 'w', encoding='utf-8') as f:
            f.write(cookie_string.strip())
        return True
    except Exception as e:
        logger.error(f"ä¿å­˜Cookieså¤±è´¥: {e}")
        return False


class WeidianScraper:
    """å¾®åº—å•†å“ä¿¡æ¯çˆ¬è™« - ä½¿ç”¨å®˜æ–¹API"""

    def __init__(self):
        self.session = requests.Session()

        # [æ–°å¢] ä¼˜åŒ–è¿æ¥æ± ï¼Œé˜²æ­¢å¤šçº¿ç¨‹æŠ“å–æ—¶è¿æ¥æ•°ä¸å¤Ÿ
        from requests.adapters import HTTPAdapter
        # è®¾ç½®è¿æ¥æ± å¤§å°ä¸º 50ï¼Œé‡è¯•æ¬¡æ•° 3
        adapter = HTTPAdapter(pool_connections=50, pool_maxsize=50, max_retries=3)
        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)

        # ä¿®å¤ï¼šæ›´æ–° Headersï¼Œå®Œå…¨åŒ¹é…ä½ çš„ CURL è¯·æ±‚
        self.session.headers.update({
            'accept': 'application/json, */*',  # æ³¨æ„ï¼šcurlä¸­æ˜¯ application/json, / ä½†å®é™…åº”è¯¥æ˜¯ /*
            'accept-language': 'en-US,en;q=0.9,zh-HK;q=0.8,zh-CN;q=0.7,zh;q=0.6',
            'origin': 'https://weidian.com',
            'priority': 'u=1, i',
            'referer': 'https://weidian.com/',
            'sec-ch-ua': '"Google Chrome";v="143", "Chromium";v="143", "Not A(Brand";v="24"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"macOS"',
            'sec-fetch-dest': 'empty',
            'sec-fetch-mode': 'cors',
            'sec-fetch-site': 'same-site',
            'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36'
        })

        # ä¿®å¤ï¼šæ›´æ–° Cookies
        self.session.cookies.update({
            'wdtoken': '8ea9315c',
            '__spider__visitorid': '0dcf6a5b878847ec',
            'visitor_id': '4d36e980-4128-451c-8178-a976b6303114',
            'v-components/cpn-coupon-dialog@nologinshop': '6',
            '__spider__sessionid': 'e0e858ac8efb20a2'
        })
        stored_cookies = load_cookie_string()
        if stored_cookies:
            self.update_cookies(stored_cookies)

    def update_cookies(self, cookie_string: str):
        """å…è®¸ç”¨æˆ·ä»å¤–éƒ¨æ›´æ–° Cookies"""
        try:
            from http.cookies import SimpleCookie

            cookie = SimpleCookie()
            cookie.load(cookie_string)
            cookies_dict = {k: v.value for k, v in cookie.items()}
            self.session.cookies.update(cookies_dict)
            logger.info("Cookies å·²æ›´æ–°")
        except Exception as e:
            logger.warning(f"æ›´æ–°Cookieså¤±è´¥: {e}")

    def _get_wdtoken(self) -> str:
        return str(self.session.cookies.get('wdtoken', '') or '')

    def _get_h5_headers(self) -> Dict[str, str]:
        return {
            'accept': 'application/json, */*',
            'accept-language': 'en-US,en;q=0.9,zh-HK;q=0.8,zh-CN;q=0.7,zh;q=0.6',
            'origin': 'https://h5.weidian.com',
            'priority': 'u=1, i',
            'referer': 'https://h5.weidian.com/',
            'sec-ch-ua': '"Not(A:Brand";v="8", "Chromium";v="144", "Google Chrome";v="144"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"macOS"',
            'sec-fetch-dest': 'empty',
            'sec-fetch-mode': 'cors',
            'sec-fetch-site': 'same-site',
            'user-agent': (
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '
                'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36'
            )
        }

    def _request_with_retry(
        self,
        url: str,
        headers: Optional[Dict] = None,
        cookies: Optional[Dict] = None,
        timeout: int = 10,
        max_retries: int = 5,
        backoff: float = 0.5
    ) -> Optional[requests.Response]:
        for attempt in range(1, max_retries + 1):
            try:
                response = self.session.get(
                    url,
                    headers=headers,
                    cookies=cookies,
                    timeout=timeout,
                    proxies={'http': None, 'https': None}
                )
                response.raise_for_status()
                return response
            except Exception as e:
                if attempt < max_retries:
                    time.sleep(backoff * attempt)
                else:
                    logger.warning(f"è¯·æ±‚å¤±è´¥({max_retries}æ¬¡): {url} | {e}")
        return None

    def _request_json_with_retry(
        self,
        url: str,
        headers: Optional[Dict] = None,
        cookies: Optional[Dict] = None,
        timeout: int = 10,
        max_retries: int = 5,
        backoff: float = 0.5
    ) -> Optional[Dict]:
        for attempt in range(1, max_retries + 1):
            try:
                response = self.session.get(
                    url,
                    headers=headers,
                    cookies=cookies,
                    timeout=timeout,
                    proxies={'http': None, 'https': None}
                )
                response.raise_for_status()
                return response.json()
            except Exception as e:
                if attempt < max_retries:
                    time.sleep(backoff * attempt)
                else:
                    logger.warning(f"JSONè¯·æ±‚å¤±è´¥({max_retries}æ¬¡): {url} | {e}")
        return None

    def _build_shop_list_urls(self, endpoint: str, shop_id: str, page: int, page_size: int) -> List[str]:
        param_variants = [
            {'pageNum': page, 'pageSize': page_size, 'shopId': str(shop_id), 'sort': 0, 'desc': 1},
            {'pageNum': page, 'pageSize': page_size, 'userid': str(shop_id), 'sort': 0, 'desc': 1},
            {'pageNum': page, 'pageSize': page_size, 'userId': str(shop_id), 'sort': 0, 'desc': 1}
        ]
        wdtoken = self.session.cookies.get('wdtoken', '')
        timestamp = int(time.time() * 1000)
        urls = []
        for param in param_variants:
            encoded_param = quote(json.dumps(param, separators=(',', ':')))
            urls.append(f"{endpoint}?param={encoded_param}&wdtoken={wdtoken}&_={timestamp}")
        return urls

    def _parse_shop_list_response(self, data: Dict) -> Dict:
        if not isinstance(data, dict):
            return {'items': [], 'total': None}

        status = data.get('status', {})
        code = status.get('code', 0)
        if code not in (0, None):
            return {'items': [], 'total': None}

        result = data.get('result') or data.get('data') or {}
        items = []
        total = None

        if isinstance(result, list):
            items = result
        else:
            items = (
                result.get('items')
                or result.get('itemList')
                or result.get('list')
                or result.get('itemsList')
                or []
            )
            total = (
                result.get('total')
                or result.get('total_count')
                or result.get('totalCount')
                or result.get('itemCount')
            )

        item_ids = []
        for item in items or []:
            if isinstance(item, dict):
                item_id = (
                    item.get('itemId')
                    or item.get('item_id')
                    or item.get('itemID')
                    or item.get('id')
                )
            else:
                item_id = None
            if item_id is not None:
                item_ids.append(str(item_id))

        if not item_ids:
            item_ids = self._extract_item_ids(data)

        return {'items': item_ids, 'total': total}

    def _extract_item_ids(self, data: object) -> List[str]:
        item_ids: List[str] = []
        keys = {'itemId', 'item_id', 'itemID', 'id'}

        def walk(value: object) -> None:
            if isinstance(value, dict):
                for key, val in value.items():
                    if key in keys and val is not None:
                        item_ids.append(str(val))
                    walk(val)
            elif isinstance(value, list):
                for item in value:
                    walk(item)

        walk(data)
        return item_ids

    def _build_cate_tree_url(self, shop_id: str) -> str:
        param = {
            'shopId': str(shop_id),
            'attrQuery': [],
            'from': 'h5'
        }
        encoded_param = quote(json.dumps(param, separators=(',', ':')))
        return (
            "https://thor.weidian.com/decorate/itemCate.getCateTree/1.0"
            f"?param={encoded_param}&wdtoken={self._get_wdtoken()}"
        )

    def _build_cate_item_list_url(self, shop_id: str, cate_id: str, offset: int, limit: int) -> str:
        param = {
            'cateId': str(cate_id),
            'shopId': str(shop_id),
            'offset': offset,
            'limit': limit,
            'sortField': 'all',
            'sortType': 'desc',
            'isQdFx': False,
            'isHideSold': False,
            'hideItemRealAmount': False,
            'from': 'h5',
            'fanSpreadMode': -1,
            'isShopItemListConfOpen': False,
            'attrQuery': [],
            'isStockDown': 0,
            'isConsumerProtect': False,
            'hideItemComment': False
        }
        encoded_param = quote(json.dumps(param, separators=(',', ':')))
        return (
            "https://thor.weidian.com/decorate/itemCate.getCateItemList/1.0"
            f"?param={encoded_param}&wdtoken={self._get_wdtoken()}"
        )

    def _fetch_shop_categories(self, shop_id: str) -> List[str]:
        url = self._build_cate_tree_url(shop_id)
        headers = self._get_h5_headers()
        data = self._request_json_with_retry(url, headers=headers, timeout=15, max_retries=3)
        if not data or not isinstance(data, dict):
            return []

        status = data.get('status', {})
        if status.get('code') not in (0, None):
            return []

        result = data.get('result') or {}
        cate_list = result.get('cateList') or []
        cate_ids = []
        for cate in cate_list:
            if isinstance(cate, dict):
                cate_id = cate.get('cateId')
                if cate_id is not None:
                    cate_ids.append(str(cate_id))
        return cate_ids

    def _fetch_cate_items_page(self, shop_id: str, cate_id: str, offset: int, limit: int) -> Dict:
        url = self._build_cate_item_list_url(shop_id, cate_id, offset, limit)
        headers = self._get_h5_headers()
        data = self._request_json_with_retry(url, headers=headers, timeout=15, max_retries=3)
        if not data or not isinstance(data, dict):
            return {'items': [], 'has_data': False}

        status = data.get('status', {})
        if status.get('code') not in (0, None):
            return {'items': [], 'has_data': False}

        result = data.get('result') or {}
        item_list = result.get('itemList') or []
        item_ids = []
        for item in item_list:
            if isinstance(item, dict):
                item_id = item.get('itemId') or item.get('itemID') or item.get('id')
                if item_id is not None:
                    item_ids.append(str(item_id))
        has_data = bool(result.get('hasData'))
        return {'items': item_ids, 'has_data': has_data}

    def _fetch_shop_items_page(self, shop_id: str, page: int, page_size: int) -> Dict:
        endpoints = [
            'https://thor.weidian.com/wdshop/getItemList/1.0',
            'https://thor.weidian.com/wdshop/getShopItemList/1.0',
            'https://thor.weidian.com/wdshop/getShopItems/1.0'
        ]
        headers = dict(self.session.headers)
        headers['referer'] = f"https://weidian.com/?userid={shop_id}"

        for endpoint in endpoints:
            for url in self._build_shop_list_urls(endpoint, shop_id, page, page_size):
                data = self._request_json_with_retry(url, headers=headers, timeout=15, max_retries=3)
                if not data:
                    continue
                parsed = self._parse_shop_list_response(data)
                if parsed['items']:
                    return parsed

        return {'items': [], 'total': None}

    def _fetch_shop_item_ids_from_html(self, shop_id: str) -> List[str]:
        url = f"https://weidian.com/?userid={shop_id}"
        html_headers = {
            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'accept-language': 'en-US,en;q=0.9,zh-HK;q=0.8,zh-CN;q=0.7,zh;q=0.6',
            'cache-control': 'max-age=0',
            'sec-ch-ua': '"Google Chrome";v="143", "Chromium";v="143", "Not A(Brand";v="24"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"macOS"',
            'sec-fetch-dest': 'document',
            'sec-fetch-mode': 'navigate',
            'sec-fetch-site': 'none',
            'sec-fetch-user': '?1',
            'upgrade-insecure-requests': '1',
            'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36'
        }

        response = self._request_with_retry(url, headers=html_headers, timeout=15, max_retries=5)
        if not response:
            return []

        html_content = response.text
        patterns = [
            r'itemID=(\d+)',
            r'"itemID"\s*:\s*"(\d+)"',
            r'"itemId"\s*:\s*"(\d+)"',
            r'"item_id"\s*:\s*"(\d+)"'
        ]
        item_ids = []
        for pattern in patterns:
            item_ids.extend(re.findall(pattern, html_content))
        return list(dict.fromkeys(item_ids))

    def fetch_shop_item_ids(self, shop_id: str, page_size: int = None, max_pages: int = None) -> List[str]:
        page_size = page_size or config.SHOP_SCRAPE_PAGE_SIZE
        max_pages = max_pages or config.SHOP_SCRAPE_MAX_PAGES

        cate_page_size = min(page_size, 20)
        cate_ids = self._fetch_shop_categories(shop_id)
        if cate_ids:
            item_ids: List[str] = []

            def fetch_category_items(cate_id: str) -> List[str]:
                collected: List[str] = []
                offset = 0
                page = 0
                while page < max_pages:
                    page_data = self._fetch_cate_items_page(shop_id, cate_id, offset, cate_page_size)
                    if not page_data['items']:
                        if not page_data['has_data']:
                            break
                        break
                    collected.extend(page_data['items'])
                    if len(page_data['items']) < cate_page_size:
                        break
                    offset += cate_page_size
                    page += 1
                return collected

            workers = max(1, config.SCRAPE_THREADS)
            with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
                futures = [executor.submit(fetch_category_items, cate_id) for cate_id in cate_ids]
                for future in concurrent.futures.as_completed(futures):
                    item_ids.extend(future.result())

            item_ids = list(dict.fromkeys(item_ids))
            if item_ids:
                return item_ids

        first_page = self._fetch_shop_items_page(shop_id, 1, page_size)
        item_ids = list(first_page['items'])

        total = first_page.get('total')
        total_pages = None
        if isinstance(total, int):
            total_pages = max(1, (total + page_size - 1) // page_size)
            total_pages = min(total_pages, max_pages)

        if total_pages is None:
            page = 2
            while page <= max_pages:
                page_data = self._fetch_shop_items_page(shop_id, page, page_size)
                if not page_data['items']:
                    break
                item_ids.extend(page_data['items'])
                page += 1
            item_ids = list(dict.fromkeys(item_ids))
            if not item_ids:
                item_ids = self._fetch_shop_item_ids_from_html(shop_id)
            return list(dict.fromkeys(item_ids))

        pages = list(range(2, total_pages + 1))
        if pages:
            workers = max(1, config.SCRAPE_THREADS)
            with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
                futures = [
                    executor.submit(self._fetch_shop_items_page, shop_id, page, page_size)
                    for page in pages
                ]
                for future in concurrent.futures.as_completed(futures):
                    data = future.result()
                    item_ids.extend(data.get('items', []))

        item_ids = list(dict.fromkeys(item_ids))
        if not item_ids:
            item_ids = self._fetch_shop_item_ids_from_html(shop_id)
        return list(dict.fromkeys(item_ids))

    def extract_item_id(self, url: str) -> Optional[str]:
        """ä»å¾®åº—URLä¸­æå–å•†å“ID"""
        try:
            parsed_url = urlparse(url)
            if 'itemID' in parsed_url.query:
                query_params = parse_qs(parsed_url.query)
                return query_params.get('itemID', [None])[0]
            else:
                # å°è¯•ä»è·¯å¾„ä¸­æå–
                path_match = re.search(r'/item/(\d+)', parsed_url.path)
                if path_match:
                    return path_match.group(1)

                # å°è¯•å…¶ä»–æ ¼å¼
                id_match = re.search(r'itemID[=/](\d+)', url)
                if id_match:
                    return id_match.group(1)

            return None
        except Exception as e:
            logger.error(f"æå–å•†å“IDå¤±è´¥: {e}")
            return None

    def scrape_product_info(self, url: str) -> Optional[Dict]:
        """
        æŠ“å–å¾®åº—å•†å“ä¿¡æ¯ - ä½¿ç”¨å®˜æ–¹API
        è¿”å›åŒ…å«æ ‡é¢˜ã€æè¿°ã€å›¾ç‰‡ç­‰ä¿¡æ¯çš„å­—å…¸
        """
        try:
            item_id = self.extract_item_id(url)
            if not item_id:
                logger.error(f"æ— æ³•ä»URLæå–å•†å“ID: {url}")
                return None

            logger.info(f"å¼€å§‹æŠ“å–å•†å“: {item_id}")

            # è·å–åº—é“ºä¿¡æ¯
            shop_name = self._get_shop_name(url)
            if shop_name == "æœªçŸ¥åº—é“º":
                logger.info("åº—é“ºåç§°è·å–å¤±è´¥ï¼Œå°è¯•ä»é¡µé¢HTMLæå–")
                try:
                    page_response = requests.get(url, timeout=10, proxies={'http': None, 'https': None}, headers={
                        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                        'accept-language': 'en-US,en;q=0.9,zh-HK;q=0.8,zh-CN;q=0.7,zh;q=0.6',
                        'cache-control': 'max-age=0',
                        'referer': 'https://weidian.com/?userid=1713062461&wfr=c&source=home_shop&ifr=itemdetail&sfr=app&tabType=all',
                        'sec-ch-ua': '"Google Chrome";v="143", "Chromium";v="143", "Not A(Brand";v="24"',
                        'sec-ch-ua-mobile': '?0',
                        'sec-ch-ua-platform': '"macOS"',
                        'sec-fetch-dest': 'document',
                        'sec-fetch-mode': 'navigate',
                        'sec-fetch-site': 'same-origin',
                        'sec-fetch-user': '?1',
                        'upgrade-insecure-requests': '1',
                        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36'
                    }, cookies={
                        'wdtoken': '8ea9315c',
                        '__spider__visitorid': '0dcf6a5b878847ec',
                        'visitor_id': '4d36e980-4128-451c-8178-a976b6303114',
                        'v-components/cpn-coupon-dialog@nologinshop': '10',
                        '__spider__sessionid': 'e55c6458ac1fdba4'
                    })

                    if page_response.status_code == 200:
                        # å°è¯•ä»JavaScriptæ•°æ®ä¸­æå–åº—é“ºåç§°
                        shop_name_pattern = r'"shopName"[^:]*:[^"]*"([^"]+)"'
                        match = re.search(shop_name_pattern, page_response.text, re.DOTALL | re.IGNORECASE)
                        if match:
                            shop_name = match.group(1).strip()
                            logger.info(f"âœ… ä»JavaScriptæ•°æ®è·å–åˆ°åº—é“ºåç§°: {shop_name}")
                except Exception as e:
                    logger.warning(f"ä»é¡µé¢æå–åº—é“ºåç§°å¤±è´¥: {e}")

            # ä½¿ç”¨å®˜æ–¹APIè·å–å•†å“ä¿¡æ¯
            product_info = self._scrape_by_api(item_id, url, shop_name)
            if product_info:
                logger.info(f"âœ… å•†å“ä¿¡æ¯æŠ“å–æˆåŠŸ: {product_info.get('title', 'Unknown')}")
                return product_info

            # å¦‚æœAPIå¤±è´¥ï¼Œè¿”å›None
            logger.error("APIæŠ“å–å¤±è´¥ï¼Œæ²¡æœ‰å¤‡ç”¨æ–¹æ³•")
            return None

        except Exception as e:
            logger.error(f"å•†å“ä¿¡æ¯æŠ“å–å¤±è´¥: {e}")
            return None

    def _scrape_by_api(self, item_id: str, url: str, shop_name: str = '') -> Optional[Dict]:
        """ä½¿ç”¨å¾®åº—å®˜æ–¹APIæŠ“å–å•†å“ä¿¡æ¯"""
        try:
            # è·å–å•†å“æ ‡é¢˜å’ŒSKUä¿¡æ¯
            title_info = self._get_item_title_and_sku(item_id)
            title = title_info.get('title', '') if title_info else ''

            # å¦‚æœAPIè·å–å¤±è´¥ï¼Œå°è¯•ä»é¡µé¢HTMLä¸­æå–å•†å“æ ‡é¢˜
            if not title:
                logger.info("APIè·å–æ ‡é¢˜å¤±è´¥ï¼Œå°è¯•ä»é¡µé¢HTMLæå–")
                try:
                    page_response = requests.get(url, timeout=10, proxies={'http': None, 'https': None}, headers={
                        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                        'accept-language': 'en-US,en;q=0.9,zh-HK;q=0.8,zh-CN;q=0.7,zh;q=0.6',
                        'cache-control': 'max-age=0',
                        'referer': 'https://weidian.com/?userid=1713062461&wfr=c&source=home_shop&ifr=itemdetail&sfr=app&tabType=all',
                        'sec-ch-ua': '"Google Chrome";v="143", "Chromium";v="143", "Not A(Brand";v="24"',
                        'sec-ch-ua-mobile': '?0',
                        'sec-ch-ua-platform': '"macOS"',
                        'sec-fetch-dest': 'document',
                        'sec-fetch-mode': 'navigate',
                        'sec-fetch-site': 'same-origin',
                        'sec-fetch-user': '?1',
                        'upgrade-insecure-requests': '1',
                        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36'
                    }, cookies={
                        'wdtoken': '8ea9315c',
                        '__spider__visitorid': '0dcf6a5b878847ec',
                        'visitor_id': '4d36e980-4128-451c-8178-a976b6303114',
                        'v-components/cpn-coupon-dialog@nologinshop': '10',
                        '__spider__sessionid': 'e55c6458ac1fdba4'
                    })

                    if page_response.status_code == 200:
                        # ä»é¡µé¢HTMLä¸­æå–å•†å“æ ‡é¢˜
                        title_pattern = r'<span[^>]*class="[^"]*item-name[^"]*"[^>]*>([^<]+)</span>'
                        match = re.search(title_pattern, page_response.text, re.DOTALL | re.IGNORECASE)
                        if match:
                            title = match.group(1).strip()
                            logger.info(f"âœ… ä»é¡µé¢HTMLè·å–åˆ°å•†å“æ ‡é¢˜: {title}")
                        else:
                            title = f'å¾®åº—å•†å“ {item_id}'
                except Exception as e:
                    logger.warning(f"ä»é¡µé¢HTMLæå–æ ‡é¢˜å¤±è´¥: {e}")
                    title = f'å¾®åº—å•†å“ {item_id}'
            else:
                title = title

            # è·å–å•†å“å›¾ç‰‡ä¿¡æ¯ï¼ˆå³ä½¿æ ‡é¢˜è·å–å¤±è´¥ä¹Ÿè¦å°è¯•è·å–å›¾ç‰‡ï¼‰
            image_info = self._get_item_images(item_id)
            images = image_info if image_info else []

            # å¦‚æœæ—¢æ²¡æœ‰æ ‡é¢˜ä¹Ÿæ²¡æœ‰å›¾ç‰‡ï¼Œè¿”å›None
            if not title and not images:
                logger.error("æ— æ³•è·å–å•†å“æ ‡é¢˜å’Œå›¾ç‰‡ä¿¡æ¯")
                return None

            # æ„å»ºå•†å“ä¿¡æ¯
            product_info = {
                'id': item_id,
                'weidian_url': url,
                'cnfans_url': f"https://cnfans.com/product?id={item_id}&platform=WEIDIAN",
                'acbuy_url': f"https://www.acbuy.com/product?url=https%253A%252F%252Fweidian.com%252Fitem.html%253FitemID%253D{item_id}%2526spider_token%253D43fe&id={item_id}&source=WD",
                'images': images,
                'title': title,
                'english_title': self._generate_english_title(title),
                'description': f"å¾®åº—å•†å“ID: {item_id}",
                'shop_name': shop_name
            }

            return product_info

        except Exception as e:
            logger.error(f"APIæŠ“å–å¤±è´¥: {e}")
            return None

    def _get_item_title_and_sku(self, item_id: str) -> Optional[Dict]:
        """è·å–å•†å“æ ‡é¢˜å’ŒSKUä¿¡æ¯"""
        try:
            # æ„é€ API URL - ä½¿ç”¨æ›´æ–°çš„æ ¼å¼
            param = json.dumps({"itemId": item_id})
            encoded_param = quote(param)
            timestamp = int(time.time() * 1000)

            api_url = f"https://thor.weidian.com/detail/getItemSkuInfo/1.0?param={encoded_param}&wdtoken=8ea9315c&_={timestamp}"

            logger.info(f"è°ƒç”¨SKU API: {api_url}")  # ä¿®æ”¹æ—¥å¿—çº§åˆ«ä¸º INFO ä»¥ä¾¿è°ƒè¯•

            # ä½¿ç”¨ä¸å‰ç«¯ fetch å®Œå…¨ä¸€è‡´çš„ headers
            headers = {
                "accept": "application/json, */*",
                "accept-language": "en-US,en;q=0.9,zh-HK;q=0.8,zh-CN;q=0.7,zh;q=0.6",
                "priority": "u=1, i",
                "sec-ch-ua": '"Google Chrome";v="143", "Chromium";v="143", "Not A(Brand";v="24"',
                "sec-ch-ua-mobile": "?0",
                "sec-ch-ua-platform": '"macOS"',
                "sec-fetch-dest": "empty",
                "sec-fetch-mode": "cors",
                "sec-fetch-site": "same-site",
                "referrer": "https://weidian.com/",
                "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36"
            }

            # ä¸å¸¦ cookies å‘é€è¯·æ±‚ (æœ‰æ—¶å€™ cookies ä¼šå¯¼è‡´æ ¡éªŒå¤±è´¥)
            response = requests.get(api_url, headers=headers, timeout=15)
            response.raise_for_status()

            data = response.json()
            logger.debug(f"æ ‡é¢˜APIè¿”å›çŠ¶æ€: {data.get('status', {}).get('code')}")

            if data.get('status', {}).get('code') == 0:
                result = data.get('result', {})
                title = result.get('itemTitle', '')
                if title:
                    return {'title': title, 'sku_info': result}

            # APIè°ƒç”¨å¤±è´¥ï¼Œè®°å½•è­¦å‘Šä½†ä¸å°è¯•HTML fallback

            return None

        except Exception as e:
            logger.error(f"è·å–å•†å“æ ‡é¢˜å¤±è´¥: {e}")
            return None

    def _get_item_images(self, item_id: str) -> List[str]:
        """è·å–å•†å“å›¾ç‰‡ä¿¡æ¯ - åŒæ—¶è°ƒç”¨ä¸¤ä¸ªAPIå¹¶å»é‡"""
        try:
            all_images = []

            # 1. è·å–å•†å“è¯¦æƒ…å›¾ç‰‡ (åŸæœ‰API)
            detail_images = self._get_detail_images(item_id)
            all_images.extend(detail_images)

            # 2. è·å–SKUå±æ€§å›¾ç‰‡ (æ–°API)
            sku_images = self._get_sku_images(item_id)
            all_images.extend(sku_images)

            # 3. ç®€å•URLå»é‡
            unique_images = []
            seen_urls = set()
            for img_url in all_images:
                if img_url and img_url not in seen_urls:
                    unique_images.append(img_url)
                    seen_urls.add(img_url)

            logger.info(f"âœ… å•†å“ {item_id} å›¾ç‰‡è·å–å®Œæˆ: å…± {len(unique_images)} å¼  (è¯¦æƒ…:{len(detail_images)}, SKU:{len(sku_images)})")
            if len(unique_images) > 0:
                logger.info(f"ğŸ“¸ å›¾ç‰‡URLæ ·ä¾‹: {unique_images[:3]}")
            return unique_images

        except Exception as e:
            logger.error(f"è·å–å•†å“å›¾ç‰‡å¤±è´¥: {e}")
            return []

    def _get_detail_images(self, item_id: str) -> List[str]:
        """è·å–å•†å“è¯¦æƒ…å›¾ç‰‡ (åŸæœ‰API)"""
        try:
            # æ„é€ API URL
            param = json.dumps({"vItemId": item_id})
            encoded_param = quote(param)
            timestamp = int(time.time() * 1000)

            api_url = f"https://thor.weidian.com/detail/getDetailDesc/1.0?param={encoded_param}&wdtoken=8ea9315c&_={timestamp}"

            logger.debug(f"è°ƒç”¨è¯¦æƒ…å›¾ç‰‡API: {api_url}")

            # ä½¿ç”¨æ›´ç¨³å®šçš„è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è¡Œä¸º
            import requests
            headers = {
                'accept': 'application/json, text/plain, */*',
                'accept-language': 'en-US,en;q=0.9,zh-HK;q=0.8,zh-CN;q=0.7,zh;q=0.6',
                'origin': 'https://weidian.com',
                'priority': 'u=1, i',
                'referer': 'https://weidian.com/',
                'sec-ch-ua': '"Google Chrome";v="143", "Chromium";v="143", "Not A(Brand";v="24"',
                'sec-ch-ua-mobile': '?0',
                'sec-ch-ua-platform': '"macOS"',
                'sec-fetch-dest': 'empty',
                'sec-fetch-mode': 'cors',
                'sec-fetch-site': 'same-site',
                'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36'
            }

            # è®¾ç½®cookies
            cookies = {
                'wdtoken': '8ea9315c',
                '__spider__visitorid': '0dcf6a5b878847ec',
                'visitor_id': '4d36e980-4128-451c-8178-a976b6303114',
                'v-components/cpn-coupon-dialog@nologinshop': '10',
                '__spider__sessionid': 'e55c6458ac1fdba4'
            }

            response = requests.get(api_url, timeout=15, proxies={'http': None, 'https': None}, headers=headers, cookies=cookies)
            response.raise_for_status()

            data = response.json()
            logger.debug(f"è¯¦æƒ…å›¾ç‰‡APIè¿”å›çŠ¶æ€: {data.get('status', {}).get('code')}")

            images = []
            if data.get('status', {}).get('code') == 0:
                item_detail = data.get('result', {}).get('item_detail', {})
                desc_content = item_detail.get('desc_content', [])

                for item in desc_content:
                    if item.get('type') == 2 and item.get('url'):
                        images.append(item['url'])

            return images

        except Exception as e:
            logger.error(f"è·å–è¯¦æƒ…å›¾ç‰‡å¤±è´¥: {e}")
            return []

    def _get_sku_images(self, item_id: str) -> List[str]:
        """è·å–SKUå±æ€§å›¾ç‰‡ (æ–°API + attrListè§£æ)"""
        try:
            logger.info(f"å¼€å§‹è·å–SKUå›¾ç‰‡ï¼Œå•†å“ID: {item_id}")
            title_info = self._get_item_title_and_sku(item_id)
            if not title_info or 'sku_info' not in title_info:
                logger.warning(f"æ— æ³•è·å–SKUä¿¡æ¯ï¼Œè·³è¿‡å›¾ç‰‡æå–: {item_id}")
                return []

            result = title_info['sku_info']
            images = []
            seen_urls = set()

            # 1. å°è¯•ä» attrList ä¸­æå– (è¿™æ˜¯ä½ æä¾›çš„JSONä¸­çš„ç»“æ„)
            attr_list = result.get('attrList', [])
            if attr_list:
                logger.info(f"è§£æ attrListï¼Œå…± {len(attr_list)} ç»„å±æ€§")
                for attr in attr_list:
                    attr_values = attr.get('attrValues', [])
                    for val in attr_values:
                        img_url = val.get('img')
                        if img_url:
                            # ä¿®å¤ URL æ ¼å¼
                            if img_url.startswith('//'):
                                img_url = 'https:' + img_url

                            if img_url not in seen_urls:
                                images.append(img_url)
                                seen_urls.add(img_url)

            # 2. å°è¯•ä» skuInfos ä¸­æå– (ä½œä¸ºè¡¥å……)
            sku_infos = result.get('skuInfos', [])
            if sku_infos:
                logger.info(f"è§£æ skuInfosï¼Œå…± {len(sku_infos)} ä¸ªSKU")
                for sku in sku_infos:
                    # æ³¨æ„ï¼šskuInfo å¯¹è±¡å¯èƒ½åµŒå¥—
                    info = sku.get('skuInfo', {})
                    img_url = info.get('img')
                    if img_url:
                        if img_url.startswith('//'):
                            img_url = 'https:' + img_url
                        if img_url not in seen_urls:
                            images.append(img_url)
                            seen_urls.add(img_url)

            logger.info(f"ä»SKUå±æ€§ä¸­æˆåŠŸæå– {len(images)} å¼ å›¾ç‰‡")
            return images
        except Exception as e:
            logger.error(f"è·å–SKUå›¾ç‰‡å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []


    def _generate_english_title(self, chinese_title: str) -> str:
        """æ ¹æ®ä¸­æ–‡æ ‡é¢˜ç”Ÿæˆè‹±æ–‡æ ‡é¢˜ - ä½¿ç”¨å…è´¹ç¿»è¯‘API"""
        if not chinese_title or len(chinese_title.strip()) == 0:
            return ""
        # ä¼˜å…ˆä½¿ç”¨ Google å…è´¹æ¥å£ï¼Œå¤±è´¥å†å›é€€åˆ°ç™¾åº¦ï¼Œå†å›é€€åˆ°ç®€å•æ˜ å°„
        try:
            return self._translate_with_google(chinese_title)
        except Exception as e:
            logger.debug(f"Google ç¿»è¯‘å¤±è´¥: {e}")
        try:
            res = self._translate_with_baidu(chinese_title)
            if res:
                return res
        except Exception as e:
            logger.debug(f"ç™¾åº¦ç¿»è¯‘å¤±è´¥: {e}")
        # æœ€åå¤‡ç”¨ï¼šç®€å•æ˜ å°„
        return self._simple_chinese_to_english(chinese_title)

    def _translate_with_baidu(self, text: str) -> str:
        """ä½¿ç”¨ç™¾åº¦ç¿»è¯‘API"""
        try:
            # ç™¾åº¦ç¿»è¯‘å…è´¹API
            url = "https://fanyi.baidu.com/transapi"

            params = {
                'from': 'zh',
                'to': 'en',
                'query': text[:200]  # é™åˆ¶é•¿åº¦
            }

            response = self.session.get(url, params=params, timeout=10, proxies={'http': None, 'https': None})
            response.raise_for_status()

            data = response.json()
            # å°è¯•å¤šç§å¯èƒ½çš„è¿”å›ç»“æ„ï¼Œé¿å…ç›´æ¥æŠ›å‡ºå¼‚å¸¸
            translated = ""
            if isinstance(data, dict):
                try:
                    translated = data.get('data', {}).get('result', [{}])[0].get('dst', '') or ''
                except Exception:
                    translated = ''
                if not translated:
                    if 'trans_result' in data:
                        try:
                            translated = data.get('trans_result', [{}])[0].get('dst', '') or ''
                        except Exception:
                            translated = ''
            if translated:
                return translated.strip()
            logger.debug("ç™¾åº¦ç¿»è¯‘è¿”å›ç©ºç»“æœ")
            return ""
        except Exception as e:
            logger.warning(f"ç™¾åº¦ç¿»è¯‘APIè°ƒç”¨å¼‚å¸¸: {e}")
            return ""

    def _translate_with_google(self, text: str) -> str:
        """ä½¿ç”¨Google Translate APIçš„å…è´¹ç‰ˆæœ¬"""
        try:
            # ä½¿ç”¨Google Translateçš„å…è´¹API
            url = "https://translate.googleapis.com/translate_a/single"

            params = {
                'client': 'gtx',
                'sl': 'zh-CN',
                'tl': 'en',
                'dt': 't',
                'q': text[:500]  # é™åˆ¶é•¿åº¦
            }

            response = self.session.get(url, params=params, timeout=10, proxies={'http': None, 'https': None})
            response.raise_for_status()

            # Googleè¿”å›çš„æ˜¯JSONæ•°ç»„
            data = response.json()
            if data and len(data) > 0 and len(data[0]) > 0:
                translated = data[0][0][0]
                if translated:
                    return translated.strip()

            raise Exception("Googleç¿»è¯‘è¿”å›ç©ºç»“æœ")

        except Exception as e:
            logger.error(f"Googleç¿»è¯‘APIè°ƒç”¨å¤±è´¥: {e}")
            raise e

    def _simple_chinese_to_english(self, text: str) -> str:
        """ç®€å•çš„ä¸­è‹±æ˜ å°„ - æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆ"""
        # ç®€å•çš„å•†å“å…³é”®è¯æ˜ å°„
        mappings = {
            'é‹': 'shoes',
            'è¿åŠ¨é‹': 'sports shoes',
            'è¢œå­': 'socks',
            'é‹å­': 'shoes',
            'è¡£æœ': 'clothes',
            'ä¸Šè¡£': 'top',
            'è£¤å­': 'pants',
            'åŒ…': 'bag',
            'åŒ…åŒ…': 'bag',
            'æ‰‹æœº': 'phone',
            'ç”µè„‘': 'computer',
            'è€³æœº': 'headphones',
            'æ‰‹è¡¨': 'watch',
            'çœ¼é•œ': 'glasses',
            'å¸½å­': 'hat',
            'ä¹¦': 'book',
            'ç©å…·': 'toy',
            'æ¸¸æˆ': 'game'
        }

        result = text
        for cn, en in mappings.items():
            result = result.replace(cn, en)

        # å¦‚æœæœ‰æ˜æ˜¾çš„å˜åŒ–ï¼Œè¿”å›ç¿»è¯‘ç»“æœï¼Œå¦åˆ™è¿”å›ç©º
        if result != text:
            return result.strip()
        else:
            return ""


    def download_images(self, image_urls: List[str], save_dir: str, item_id: str) -> List[str]:
        """å¤šçº¿ç¨‹ä¸‹è½½å•†å“å›¾ç‰‡åˆ°æœ¬åœ°"""
        import os
        import concurrent.futures
        import threading

        saved_paths = []
        os.makedirs(save_dir, exist_ok=True)

        # ç§»é™¤å›¾ç‰‡æ•°é‡é™åˆ¶ï¼ŒæŠ“å–æ‰€æœ‰å¯ç”¨çš„å›¾ç‰‡
        # SKUå›¾ç‰‡é€šå¸¸æ’åœ¨è¯¦æƒ…å›¾ä¹‹åï¼Œç°åœ¨å¯ä»¥è·å–æ‰€æœ‰å›¾ç‰‡
        logger.info(f"å‡†å¤‡ä¸‹è½½ {len(image_urls)} å¼ å›¾ç‰‡ï¼ˆæ— æ•°é‡é™åˆ¶ï¼‰")

        def download_single_image(args):
            """ä¸‹è½½å•å¼ å›¾ç‰‡çš„å‡½æ•°"""
            i, img_url = args
            try:
                # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„session
                thread_session = requests.Session()
                thread_session.headers.update(self.session.headers)
                thread_session.cookies.update(self.session.cookies)

                response = thread_session.get(img_url, timeout=10, proxies={'http': None, 'https': None})
                response.raise_for_status()

                # ä¿å­˜å›¾ç‰‡
                img_path = os.path.join(save_dir, f"{item_id}_{i}.jpg")
                with open(img_path, 'wb') as f:
                    f.write(response.content)

                logger.info(f"å›¾ç‰‡ä¸‹è½½æˆåŠŸ: {img_path}")
                return img_path

            except Exception as e:
                logger.warning(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥ {img_url}: {e}")
                return None

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘ä¸‹è½½å›¾ç‰‡
        try:
            from config import config
        except ImportError:
            from .config import config
        max_workers = min(config.DOWNLOAD_THREADS, len(image_urls))  # ä½¿ç”¨é…ç½®çš„ä¸‹è½½çº¿ç¨‹æ•°

        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä¸‹è½½ä»»åŠ¡
            future_to_image = {
                executor.submit(download_single_image, (i, img_url)): (i, img_url)
                for i, img_url in enumerate(image_urls)
            }

            # æ”¶é›†ç»“æœ
            for future in concurrent.futures.as_completed(future_to_image):
                result = future.result()
                if result:
                    saved_paths.append(result)

        # æŒ‰ç´¢å¼•æ’åºç»“æœ
        saved_paths.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))

        return saved_paths

    def _get_shop_name(self, url: str) -> str:
        """ä»å•†å“é¡µé¢è·å–åº—é“ºåç§°"""
        try:
            logger.debug(f"å¼€å§‹è·å–åº—é“ºåç§°: {url}")

            # ä½¿ç”¨ä¸“é—¨çš„HTMLè¯·æ±‚headersï¼ˆä¸åŒäºAPIè¯·æ±‚çš„headersï¼‰
            html_headers = {
                'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'accept-language': 'en-US,en;q=0.9,zh-HK;q=0.8,zh-CN;q=0.7,zh;q=0.6',
                'cache-control': 'max-age=0',
                'sec-ch-ua': '"Google Chrome";v="143", "Chromium";v="143", "Not A(Brand";v="24"',
                'sec-ch-ua-mobile': '?0',
                'sec-ch-ua-platform': '"macOS"',
                'sec-fetch-dest': 'document',
                'sec-fetch-mode': 'navigate',
                'sec-fetch-site': 'none',
                'sec-fetch-user': '?1',
                'upgrade-insecure-requests': '1',
                'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36'
            }

            # è¯·æ±‚å•†å“é¡µé¢ï¼ˆä½¿ç”¨HTMLä¸“ç”¨headersï¼‰
            response = self._request_with_retry(url, headers=html_headers, timeout=10, max_retries=5)
            if not response:
                return "æœªçŸ¥åº—é“º"

            # è§£ç HTMLå®ä½“ï¼ˆ&#34; -> " ç­‰ï¼‰
            html_content = response.text
            html_content = html_content.replace('&#34;', '"').replace('&#39;', "'").replace('&quot;', '"')

            # é¦–å…ˆå°è¯•æœ€ç²¾ç¡®çš„åŒ¹é…ï¼šemæ ‡ç­¾ä¸­çš„shop-name-strç±»ï¼ˆæ ¹æ®ç”¨æˆ·æä¾›çš„HTMLç»“æ„ï¼‰
            shop_name_pattern1 = r'<em[^>]*class="[^"]*\bshop-name-str\b[^"]*"[^>]*>([^<]+)</em>'
            match = re.search(shop_name_pattern1, html_content, re.DOTALL | re.IGNORECASE)
            if match:
                shop_name = match.group(1).strip()
                logger.info(f"âœ… è·å–åˆ°åº—é“ºåç§° (em shop-name-str): {shop_name}")
                return shop_name

            # ç„¶åå°è¯•æ›´å®½æ³›çš„åŒ¹é…ï¼ŒæŸ¥æ‰¾åŒ…å«shop-name-strç±»çš„ä»»ä½•å…ƒç´ 
            shop_name_pattern2 = r'<[^>]*class="[^"]*\bshop-name-str\b[^"]*"[^>]*>([^<]+)</[^>]*>'
            match = re.search(shop_name_pattern2, html_content, re.DOTALL | re.IGNORECASE)
            if match:
                shop_name = match.group(1).strip()
                logger.info(f"âœ… è·å–åˆ°åº—é“ºåç§° (é€šç”¨shop-name-str): {shop_name}")
                return shop_name

            # å°è¯•åŒ¹é…class="shop-name-str"çš„å…ƒç´ ï¼ˆä¸é™å®šæ ‡ç­¾ç±»å‹ï¼‰
            shop_name_pattern3 = r'class="shop-name-str"[^>]*>([^<]+)</'
            match = re.search(shop_name_pattern3, html_content, re.DOTALL | re.IGNORECASE)
            if match:
                shop_name = match.group(1).strip()
                logger.info(f"âœ… è·å–åˆ°åº—é“ºåç§° (shop-name-str): {shop_name}")
                return shop_name

            # å°è¯•ä»JavaScriptæ•°æ®ä¸­æå–åº—é“ºåç§°ï¼ˆå¤šç§æ ¼å¼ï¼‰
            # æ ¼å¼1: "shopName":"Aiseo"
            shop_name_pattern4 = r'"shopName"\s*:\s*"([^"]+)"'
            match = re.search(shop_name_pattern4, html_content, re.DOTALL | re.IGNORECASE)
            if match:
                shop_name = match.group(1).strip()
                logger.info(f"âœ… è·å–åˆ°åº—é“ºåç§° (JavaScript): {shop_name}")
                return shop_name

            # æ ¼å¼2: \"shopName\":\"Aiseo\" (åœ¨HTMLä¸­è¢«è½¬ä¹‰)
            shop_name_pattern5 = r'\\"shopName\\"\s*:\s*\\"([^\\"]+)\\"'
            match = re.search(shop_name_pattern5, html_content, re.DOTALL | re.IGNORECASE)
            if match:
                shop_name = match.group(1).strip()
                logger.info(f"âœ… è·å–åˆ°åº—é“ºåç§° (JavaScriptè½¬ä¹‰): {shop_name}")
                return shop_name

            # æ ¼å¼3: shopName:"Aiseo" (æ— å¼•å·)
            shop_name_pattern6 = r'shopName\s*:\s*"([^"]+)"'
            match = re.search(shop_name_pattern6, html_content, re.DOTALL | re.IGNORECASE)
            if match:
                shop_name = match.group(1).strip()
                logger.info(f"âœ… è·å–åˆ°åº—é“ºåç§° (JavaScriptæ— å¼•å·): {shop_name}")
                return shop_name

            logger.warning("æœªæ‰¾åˆ°åº—é“ºåç§°ï¼Œä½¿ç”¨é»˜è®¤åç§°")
            return "æœªçŸ¥åº—é“º"

        except Exception as e:
            logger.error(f"è·å–åº—é“ºåç§°å¤±è´¥: {e}")
            return "æœªçŸ¥åº—é“º"

    def get_shop_name_by_shop_id(self, shop_id: str) -> str:
        """é€šè¿‡åº—é“ºIDè·å–åº—é“ºåç§°"""
        url = f"https://weidian.com/?userid={shop_id}"
        return self._get_shop_name(url)

    def close(self):
        """å…³é—­èµ„æº - å ä½æ–¹æ³•"""
        pass

# å…¨å±€çˆ¬è™«å®ä¾‹
_scraper = None

def get_weidian_scraper() -> WeidianScraper:
    """è·å–å¾®åº—çˆ¬è™«å®ä¾‹"""
    global _scraper
    if _scraper is None:
        _scraper = WeidianScraper()
    return _scraper
